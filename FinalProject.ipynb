{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbYV-x6IvNLO"
      },
      "source": [
        "Imports necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiM3IFMTuxab"
      },
      "source": [
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "import nltk\n",
        "import pickle\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJnhQkpdgdHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b525a364-a0fa-4950-af98-11a7c1581460"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBnW_enIvR6C"
      },
      "source": [
        "Mounts Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayX53S5cvEH9",
        "outputId": "d513e2a3-5e16-4bb0-84b2-1688dff750f5"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkKpBZ2yxfGb"
      },
      "source": [
        "Creates threeStageAlgorithm function and getDeltaQ helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLCj8n7NxeYr"
      },
      "source": [
        "def getDeltaQ(key1, key2, communities, node2degrees, graph, m):\n",
        "  deltaQ = 0\n",
        "  for vi in communities[key1]:\n",
        "    neighbors = set(graph.adj[vi])\n",
        "    for vj in communities[key2]:\n",
        "      aij = 1 if vj in neighbors else 0\n",
        "      deltaQ += (aij - node2degrees[vi]*node2degrees[vj]/(2*m))\n",
        "  return deltaQ / (2*m)\n",
        "\n",
        "# Takes in an undirected graph, and returns a dictionary where each key is a\n",
        "# central node of a community and each value is a list of nodes in that\n",
        "# community\n",
        "def threeStageAlgorithm(graph, saveDistances=False, pathToDistances=None, saveSimilarities=False, pathToSimilarities=None):\n",
        "  # Gets the number of nodes along with the degree of each node\n",
        "  minNode = min(graph.nodes)\n",
        "  maxNode = max(graph.nodes)\n",
        "  degrees = [(k,v) for k,v in graph.degree]\n",
        "  degrees.sort(key=lambda x : x[1],reverse=True)\n",
        "  node2degrees = {k:v for k,v in degrees}\n",
        "  n = len(degrees)\n",
        "\n",
        "  # Creates an array of all distances between pairs of nodes if necessary\n",
        "  if pathToDistances == None:\n",
        "    distances = [[None for _ in range(n+1)] for _ in range(n+1)]\n",
        "    for u in tqdm(range(minNode, maxNode), leave=False, desc='Creating distances array'):\n",
        "      for v in range(u+1,maxNode+1):\n",
        "        try:\n",
        "          pathLength = len(nx.shortest_path(graph, u, v))-1\n",
        "          distances[u][v] = pathLength\n",
        "          distances[v][u] = pathLength\n",
        "        except:\n",
        "          continue\n",
        "    \n",
        "    if saveDistances:\n",
        "      with open(pathToDistances, 'wb') as f:\n",
        "        pickle.dump(distances, f)\n",
        "  \n",
        "  # Read in an array of the distances between each pair of nodes\n",
        "  else:\n",
        "    with open(pathToDistances, 'rb') as f:\n",
        "      distances = pickle.load(f)\n",
        "\n",
        "  # Finds the number of pairs that are not connected\n",
        "  notConnected = 0\n",
        "  for u in range(minNode,minNode):\n",
        "    for v in range(u+1,maxNode+1):\n",
        "      if distances[u][v] == None:\n",
        "        notConnected += 1\n",
        "  \n",
        "  connected = n*(n-1) - notConnected\n",
        "\n",
        "  # Finds the sum of the distances\n",
        "  sumDists = 0\n",
        "  for u in range(minNode,maxNode):\n",
        "    for v in range(u+1,maxNode+1):\n",
        "      if distances[u][v] != None:\n",
        "        sumDists += distances[u][v]\n",
        "\n",
        "  # Finds the average distance between every pair of connected nodes\n",
        "  D = 2*sumDists/connected\n",
        "\n",
        "  # Selected all the central nodes and places them in set C0\n",
        "  C0 = set([degrees[0][0]])\n",
        "  for vj, _ in tqdm(degrees[1:], leave=False, desc='Finding central nodes'):\n",
        "    for v in C0:\n",
        "      if distances[v][vj] == None or distances[v][vj] >= D:\n",
        "        C0.add(vj)\n",
        "        break\n",
        "  \n",
        "  # Creates the dictionary of communities\n",
        "  communities = {n:[n] for n in C0}\n",
        "\n",
        "  # Creates an array of similarities between every pair of nodes in a graph if\n",
        "  # necessary\n",
        "  if pathToSimilarities == None:\n",
        "    similarities = [[None for _ in range(n+1)] for _ in range(n+1)]\n",
        "    for u in tqdm(range(minNode,maxNode), leave=False, desc='Creating similarities array'):\n",
        "      uNeighbors = graph.adj[u]\n",
        "      uDegree = node2degrees[u]\n",
        "      for v in range(u+1,maxNode+1):\n",
        "        vNeighbors = graph.adj[v]\n",
        "        vDegree = node2degrees[v]\n",
        "        if (uDegree + vDegree) > 0:\n",
        "          similarities[u][v] = 2*len(list(set(uNeighbors).intersection(set(vNeighbors))))/(uDegree + vDegree)\n",
        "          similarities[v][u] = similarities[u][v]\n",
        "    if saveSimilarities:\n",
        "      with open(pathToSimilarities, 'wb') as f:\n",
        "        pickle.dump(similarities, f)\n",
        "\n",
        "  # Reads in an array of similarities between every pair of nodes in a graph\n",
        "  else:\n",
        "    with open(pathToSimilarities,'rb') as f:\n",
        "      similarities = pickle.load(f)\n",
        "\n",
        "  # Adds each node that has not been labeled yet to a pre-community\n",
        "  notLabeled = set(list(range(minNode,maxNode+1))) - C0\n",
        "  for _ in tqdm(range(len(list(notLabeled))), leave=False, desc='Adding nodes to communities'):\n",
        "    maxSim = 0\n",
        "    for v in C0:\n",
        "      for vj in notLabeled:\n",
        "        if similarities[v][vj] != None and similarities[v][vj] > maxSim:\n",
        "          bestCentralNode = v\n",
        "          bestNewNode = vj\n",
        "          maxSim = similarities[v][vj]\n",
        "    communities[bestCentralNode].append(vj)\n",
        "    notLabeled.remove(vj)\n",
        "\n",
        "  # Deletes distances and similarities arrays because they will no longer be\n",
        "  # used (and they're usually very large)\n",
        "  del distances\n",
        "  del similarities\n",
        "\n",
        "  # Calculates the average degree of every node divided by 2\n",
        "  m = 0\n",
        "  for v in range(minNode,maxNode+1):\n",
        "    m += node2degrees[v]\n",
        "  m /= 2\n",
        "\n",
        "  # Calculates the change in modularity we would get if we merged each pair of\n",
        "  # communities\n",
        "  deltaQs = []\n",
        "  keys = list(communities.keys())\n",
        "  merged = {k:False for k in keys}\n",
        "  for i in tqdm(range(len(keys)-1),leave=False, desc='Calculating changes in modularities'):\n",
        "    key1 = keys[i]\n",
        "    for j in range(i+1, len(keys)):\n",
        "      key2 = keys[j]\n",
        "      deltaQ = getDeltaQ(key1, key2, communities, node2degrees, graph, m)\n",
        "      if deltaQ >= 0:\n",
        "        deltaQs.append([deltaQ, key1, key2])\n",
        "\n",
        "  # Merges communities to maximize modularity\n",
        "  deltaQs.sort(key=lambda x : x[0], reverse=True)\n",
        "  length = len(deltaQs)\n",
        "  idx = 0\n",
        "  for idx in tqdm(range(length), leave=False, desc='Merging communities'):\n",
        "    key1 = deltaQs[idx][1]\n",
        "    key2 = deltaQs[idx][2]\n",
        "    if not(merged[key1]) and not(merged[key2]):\n",
        "      merged[key1] = True\n",
        "      merged[key2] = True\n",
        "      communities[key1] = communities[key1] + communities[key2]\n",
        "      communities.pop(key2)\n",
        "  \n",
        "  new_communities = {}\n",
        "  for idx, item in enumerate(communities.items()):\n",
        "    new_communities[idx] = item[1]\n",
        "\n",
        "  return new_communities"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgylHqEmvUS6"
      },
      "source": [
        "Reads docmap file and generates a list of all documents. Each document is represented as a list with 3 elements: the node index, the document id, and the document name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaBDrXPcvG6W"
      },
      "source": [
        "docMapPath = 'drive/My Drive/CS 6850 Final Project/Data/docmap'\n",
        "file=open(docMapPath,\"r\")\n",
        "docMapStrings = file.read().split('\\n')\n",
        "docMapStrings = list(map(lambda x : x.split('\\t'), docMapStrings))\n",
        "docMap = []\n",
        "for index, line in enumerate(docMapStrings):\n",
        "  docMap.append([index+1, int(line[0]), line[1]])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf6NlhiwvhEH"
      },
      "source": [
        "Reads pair_doc file and generate a list of all the edges. Each edge is represented as a list of 2 elements: the source node and the destination node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqPscqX7vIiQ"
      },
      "source": [
        "edgesPath = 'drive/My Drive/CS 6850 Final Project/Data/pair_doc'\n",
        "file=open(edgesPath,\"r\")\n",
        "edgesStrings = file.read().split('\\n')\n",
        "edgesStrings = list(map(lambda x : x.split(' '), edgesStrings))\n",
        "edgesStrings=  list(filter(lambda x : x != [''], edgesStrings))\n",
        "edges = []\n",
        "for line in edgesStrings:\n",
        "  edges.append([int(line[0]), int(line[1])])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdSiBTxevk6f"
      },
      "source": [
        "Checks if the node indices are 0-indexed or 1-indexed by seeing if 0 is used in the edges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAQtEYxxvJM9"
      },
      "source": [
        "for edge in edges:\n",
        "  if 0 in edge:\n",
        "    print(edge)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF1uVp2dvnxA"
      },
      "source": [
        "Creates a directed graph, directedG, and an undirected graph, undirectedG1. directedG will contains all edges in the original dataset. undirectedG1 will only contain edges between nodes u and v if there is one edge going from u to v and one edge going from v to u in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C_1QFUPvoma"
      },
      "source": [
        "directedG = nx.DiGraph()\n",
        "undirectedG1 = nx.Graph()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK-Vn6eIvqc5"
      },
      "source": [
        "Adds all nodes to both graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPfVJwv3vsts"
      },
      "source": [
        "for doc in docMap:\n",
        "  directedG.add_node(doc[0], id=doc[1], name=doc[2])\n",
        "  undirectedG1.add_node(doc[0], id=doc[1], name=doc[2])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiAmXwMZvvE_"
      },
      "source": [
        "Adds all edges to directedG, and adds necessary edges to undirectedG1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3A9S6-UvxL5"
      },
      "source": [
        "edgesSeen = set()\n",
        "for edge in edges:\n",
        "  u = edge[0]\n",
        "  v = edge[1]\n",
        "  directedG.add_edge(u, v)\n",
        "  if str([v, u]) in edgesSeen:\n",
        "    undirectedG1.add_edge(u, v)\n",
        "  else:\n",
        "    edgesSeen.add(str(edge))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJAgsgyzv1ML"
      },
      "source": [
        "Creates another undirected graph, undirectedG2. This graph contains all the edges from directedG, except that they have been converted to undirected edges, and duplicate undirected edges between nodes were removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCT22hCLv13q"
      },
      "source": [
        "undirectedG2 = directedG.to_undirected()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCIoaXy5v3tY",
        "outputId": "ff2a811a-21de-46bd-9a4e-e431be7525f6"
      },
      "source": [
        "print(directedG.number_of_edges(), undirectedG1.number_of_edges() + undirectedG2.number_of_edges())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22432 22432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDZmXcOkv6c4"
      },
      "source": [
        "Makes sure each graph has the correct number of edges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdHuxscrv81Q"
      },
      "source": [
        "edgesSeen = set()\n",
        "undirectedG1Edges = undirectedG1.edges\n",
        "undirectedG2Edges = undirectedG2.edges\n",
        "for edge in directedG.edges:\n",
        "  u = edge[0]\n",
        "  v = edge[1]\n",
        "  assert (edge in undirectedG2Edges)\n",
        "  if str((v,u)) in edgesSeen:\n",
        "    assert (edge in undirectedG1Edges)\n",
        "  else:\n",
        "    edgesSeen.add(str(edge))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSB2QzkmySHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286d05aa-1fa0-4bf7-b5e2-ab870c744b8e"
      },
      "source": [
        "notConnected = 0\n",
        "for node in undirectedG2.nodes:\n",
        "  if set() == set(undirectedG2.adj[node]):\n",
        "    notConnected+= 1\n",
        "print(notConnected)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwd9La_xwtOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd355993-ba2d-44f4-e98e-c7035143f352"
      },
      "source": [
        "layers = [undirectedG2]\n",
        "curCommunities = threeStageAlgorithm(undirectedG2, pathToDistances='drive/My Drive/CS 6850 Final Project/Data/distances.pkl', pathToSimilarities='drive/My Drive/CS 6850 Final Project/Data/similarities.pkl')\n",
        "communities = [curCommunities]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3jP6X8812Yx"
      },
      "source": [
        "def generateLayerGraph(communities, childGraph):\n",
        "  graph = nx.Graph()\n",
        "  keys = list(communities.keys())\n",
        "  for k in keys:\n",
        "    graph.add_node(k)\n",
        "  \n",
        "  length = len(keys)\n",
        "  for u in range(length-1):\n",
        "    key1 = keys[u]\n",
        "    uNodes = set(communities[key1])\n",
        "    for v in range(u+1,length):\n",
        "      key2 = keys[v]\n",
        "      for vNode in communities[key2]:\n",
        "        vNeighbors = set(childGraph.adj[vNode])\n",
        "        if uNodes.intersection(vNeighbors) != set():\n",
        "          graph.add_edge(u,v)\n",
        "          break\n",
        "  \n",
        "  return graph"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3YNVN3SYu-p"
      },
      "source": [
        "layers.append(generateLayerGraph(communities[0], undirectedG2))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig3Q4g2gZcgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22008a9-5b8b-4f94-cf56-bf85010773c4"
      },
      "source": [
        "prevLayerLength = len(list(layers[0].nodes))\n",
        "iteration = 0\n",
        "while len(communities[-1].items()) > 1 and len(communities[-1].items()) < prevLayerLength:\n",
        "  iteration += 1\n",
        "  print(iteration)\n",
        "  prevLayerLength = len(list(layers[-1].nodes))\n",
        "  communities.append(threeStageAlgorithm(layers[-1]))\n",
        "  layers.append(generateLayerGraph(communities[-1], layers[-1]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVYEGB36Y_tJ"
      },
      "source": [
        "depth = len(layers) - 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyUpUwServ4c"
      },
      "source": [
        "totalNodes = 0\n",
        "for graph in layers:\n",
        "  print(len(list(graph.nodes)))\n",
        "  totalNodes += len(list(graph.nodes))\n",
        "print(totalNodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BewZgDcRLTy0"
      },
      "source": [
        "Creates a function that takes in a list of layers and communitites an creates a tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QDo3qsvLTEq"
      },
      "source": [
        "def combineLayers(layers, communities):\n",
        "  graph = nx.DiGraph()\n",
        "  n = len(layers)\n",
        "  root = str((n, 0))\n",
        "  graph.add_node(root)\n",
        "  layers.reverse()\n",
        "  communities.reverse()\n",
        "  treeLayer = n\n",
        "  for idx, curGraph in enumerate(layers[1:-1]):\n",
        "    treeLayer = treeLayer - 1\n",
        "    for node in curGraph.nodes:\n",
        "      graph.add_node(str((treeLayer, node)))\n",
        "\n",
        "    curCommunities= communities[idx]\n",
        "    for k, v in curCommunities.items():\n",
        "      for node in v:\n",
        "        graph.add_edge(str((treeLayer+1, k)), str((treeLayer, node)))\n",
        "    \n",
        "  for node in layers[-1].nodes:\n",
        "    graph.add_node(node)\n",
        "  \n",
        "  curCommunities = communities[-1]\n",
        "  for k, v in curCommunities.items():\n",
        "    for node in v:\n",
        "      graph.add_edge(str((2, k)), node)\n",
        "\n",
        "  return graph, root"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE-LnCVTWJZn"
      },
      "source": [
        "tree, root = combineLayers(layers, communities)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub1Dxy8gf7Qa"
      },
      "source": [
        "with open('drive/My Drive/CS 6850 Final Project/Data/TSAtree.pkl', 'wb') as f:\n",
        "  pickle.dump(tree, f)\n",
        "with open('drive/My Drive/CS 6850 Final Project/Data/TSARoot.pkl', 'wb') as f:\n",
        "  pickle.dump(root, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEeNJQKqYGfU",
        "outputId": "f8a36846-7707-4bf6-c1a2-7002c814c0e5"
      },
      "source": [
        "totalBranches = 0\n",
        "for node in tree.nodes:\n",
        "  totalBranches += len(list(tree.successors(node)))\n",
        "averageBranchingFactor = totalBranches/(totalNodes-len(list(undirectedG2.nodes)))\n",
        "print(averageBranchingFactor)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8404694126532273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MmyDwIWY9Fz",
        "outputId": "618d8174-e3cb-40c7-ed19-3eefb28ea592"
      },
      "source": [
        "names = list(map(lambda x : x[2].replace('_', ' '), docMap))\n",
        "print(names[:5])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Computer science', 'Topic outline of computer science', 'Computer scientist', 'Program (mathematical object)', 'Overlapping subproblem']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSMafHiOoCiL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGQ_XS-8bjPk",
        "outputId": "36f74649-b7a0-46ca-cdc3-856eee4b0969"
      },
      "source": [
        "totalWords = 0\n",
        "words = []\n",
        "for name in names:\n",
        "  word = wn.synsets(name)\n",
        "  if len(word) == 1:\n",
        "    print(name)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Informatics\n",
            "Algorithm\n",
            "Database\n",
            "Password\n",
            "Recursion\n",
            "Serialization\n",
            "Identifier\n",
            "TRIPOS\n",
            "Deadlock\n",
            "COMAL\n",
            "Gambas\n",
            "Usability\n",
            "Locale\n",
            "Flowchart\n",
            "EECS\n",
            "Scrolling\n",
            "ROAM\n",
            "Workspace\n",
            "KAON\n",
            "PRIMOS\n",
            "Quiesce\n",
            "Newsvendor\n",
            "Pintos\n",
            "CapROS\n",
            "CVSS\n",
            "GeckOS\n",
            "MoL\n",
            "Snarfing\n",
            "Thumbnail\n",
            "Byte\n",
            "Biostatistics\n",
            "Cybernetics\n",
            "Client-server\n",
            "COBOL\n",
            "Cyborg\n",
            "Concept\n",
            "Cartography\n",
            "Complexity\n",
            "Cyberspace\n",
            "Checksum\n",
            "Decipherment\n",
            "Ergonomics\n",
            "Emoticon\n",
            "Encryption\n",
            "Espionage\n",
            "Fortran\n",
            "FIFO\n",
            "GNU\n",
            "Hexadecimal\n",
            "Intension\n",
            "Kludge\n",
            "Kilobit\n",
            "MUMPS\n",
            "Microcode\n",
            "MS-DOS\n",
            "Minicomputer\n",
            "Octal\n",
            "Operand\n",
            "Oxymoron\n",
            "Prolog\n",
            "Pixel\n",
            "Programmer\n",
            "Proteome\n",
            "Retronym\n",
            "Robot\n",
            "Spyware\n",
            "Thesaurus\n",
            "Trackball\n",
            "Unix\n",
            "WYSIWYG\n",
            "Supercomputer\n",
            "Intranet\n",
            "Pseudonym\n",
            "Subroutine\n",
            "Decrypt\n",
            "Handshaking\n",
            "Provisioning\n",
            "Robotics\n",
            "Wavelet\n",
            "Proteomics\n",
            "Mebibyte\n",
            "Workstation\n",
            "Metonymy\n",
            "KRYPTON\n",
            "Webcam\n",
            "Gibibyte\n",
            "Tebibyte\n",
            "Exbibyte\n",
            "Kibibyte\n",
            "Homophone\n",
            "TWAIN\n",
            "Polysemy\n",
            "Laptop\n",
            "Pleonasm\n",
            "TADS\n",
            "Backtracking\n",
            "Nomogram\n",
            "Hemodynamics\n",
            "Allusion\n",
            "Telerobotics\n",
            "Humanoid\n",
            "Parsing\n",
            "Trigram\n",
            "Debugging\n",
            "Megabit\n",
            "Pebibyte\n",
            "Gigabit\n",
            "Mousepad\n",
            "Self-destruct\n",
            "HighLife\n",
            "PaX\n",
            "Dependability\n",
            "Touchscreen\n",
            "Terabit\n",
            "Petabit\n",
            "Exabit\n",
            "Zettabit\n",
            "Yottabit\n",
            "Kibibit\n",
            "Mebibit\n",
            "Gibibit\n",
            "Tebibit\n",
            "Pebibit\n",
            "Exbibit\n",
            "Biochip\n",
            "TYRO\n",
            "Sparse\n",
            "Sobriquet\n",
            "WVS\n",
            "IGES\n",
            "Bigram\n",
            "Prolixity\n",
            "JasPer\n",
            "HeliOS\n",
            "Cybercrime\n",
            "Multicollinearity\n",
            "Agnomen\n",
            "Cyberarts\n",
            "Presupposition\n",
            "Misconception\n",
            "Invective\n",
            "Substring\n",
            "Yobibyte\n",
            "Minify\n",
            "Zebibyte\n",
            "Pbit\n",
            "Snit\n",
            "EMBOSS\n",
            "PurVIEW\n",
            "ALOPEX\n",
            "LIFO\n",
            "Linux\n",
            "Semasiology\n",
            "LANSA\n",
            "MAPPER\n",
            "Ascendency\n",
            "HAREM\n",
            "LOLITA\n",
            "OZONE\n",
            "PYTHIA\n",
            "MUMmer\n",
            "Instantiations\n",
            "Discriminator\n",
            "Swatting\n",
            "FishEye\n",
            "Metadata\n",
            "Tux\n",
            "SLOB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyMZ37dGntkG",
        "outputId": "929e70f9-5cd0-470c-e094-aae728f10ac6"
      },
      "source": [
        "print(words)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('information_science.n.01'), Synset('algorithm.n.01'), Synset('database.n.01'), Synset('password.n.01'), Synset('recursion.n.01'), Synset('serialization.n.01'), Synset('identifier.n.01'), Synset('tripos.n.01'), Synset('deadlock.n.01'), Synset('comate.s.02'), Synset('viola_da_gamba.n.01'), Synset('serviceability.n.01'), Synset('venue.n.01'), Synset('flow_chart.n.01'), Synset('european_union.n.01'), Synset('scroll.v.01'), Synset('roll.v.12'), Synset('workspace.n.01'), Synset('kaon.n.01'), Synset('primo.n.01'), Synset('quieten.v.01'), Synset('newsagent.n.01'), Synset('pinto.n.01'), Synset('capros.n.01'), Synset('curriculum_vitae.n.01'), Synset('gecko.n.01'), Synset('gram_molecule.n.01'), Synset('pilfer.v.01'), Synset('thumbnail.n.01'), Synset('byte.n.01'), Synset('biometrics.n.01'), Synset('cybernetics.n.01'), Synset('client-server.a.01'), Synset('cobol.n.01'), Synset('cyborg.n.01'), Synset('concept.n.01'), Synset('mapmaking.n.01'), Synset('complexity.n.01'), Synset('internet.n.01'), Synset('checksum.n.01'), Synset('decoding.n.01'), Synset('biotechnology.n.02'), Synset('emoticon.n.01'), Synset('encoding.n.01'), Synset('espionage.n.01'), Synset('fortran.n.01'), Synset('first_in_first_out.n.01'), Synset('gnu.n.01'), Synset('hexadecimal.a.01'), Synset('intension.n.01'), Synset('kludge.n.01'), Synset('kilobit.n.01'), Synset('mumps.n.01'), Synset('firmware.n.01'), Synset('ms-dos.n.01'), Synset('minicomputer.n.01'), Synset('octal.a.01'), Synset('operand.n.01'), Synset('oxymoron.n.01'), Synset('prolog.n.01'), Synset('pixel.n.01'), Synset('programmer.n.01'), Synset('proteome.n.01'), Synset('retronym.n.01'), Synset('automaton.n.02'), Synset('spyware.n.01'), Synset('thesaurus.n.01'), Synset('trackball.n.01'), Synset('unix.n.01'), Synset('wysiwyg.a.01'), Synset('supercomputer.n.01'), Synset('intranet.n.01'), Synset('pseudonym.n.01'), Synset('routine.n.03'), Synset('decode.v.01'), Synset('handshake.n.01'), Synset('provision.v.01'), Synset('robotics.n.01'), Synset('ripple.n.01'), Synset('proteomics.n.01'), Synset('megabyte.n.02'), Synset('workstation.n.01'), Synset('metonymy.n.01'), Synset('krypton.n.01'), Synset('webcam.n.01'), Synset('gigabyte.n.02'), Synset('terabyte.n.02'), Synset('exabyte.n.02'), Synset('kilobyte.n.02'), Synset('homophone.n.01'), Synset('couple.n.04'), Synset('polysemy.n.01'), Synset('laptop.n.01'), Synset('pleonasm.n.01'), Synset('tad.n.01'), Synset('backtrack.v.01'), Synset('nomogram.n.01'), Synset('hemodynamics.n.01'), Synset('allusion.n.01'), Synset('telerobotics.n.01'), Synset('android.n.01'), Synset('parse.v.01'), Synset('trigram.n.01'), Synset('debug.v.01'), Synset('megabit.n.01'), Synset('petabyte.n.02'), Synset('gigabit.n.01'), Synset('mousepad.n.01'), Synset('self-destruct.v.01'), Synset('extravagance.n.03'), Synset('pax.n.01'), Synset('dependability.n.01'), Synset('touch_screen.n.01'), Synset('terabit.n.01'), Synset('petabit.n.01'), Synset('exabit.n.01'), Synset('zettabit.n.01'), Synset('yottabit.n.01'), Synset('kibibit.n.01'), Synset('mebibit.n.01'), Synset('gibibit.n.01'), Synset('tebibit.n.01'), Synset('pebibit.n.01'), Synset('exbibit.n.01'), Synset('biochip.n.01'), Synset('novice.n.02'), Synset('sparse.s.01'), Synset('nickname.n.01'), Synset('west_virginia.n.01'), Synset('immunoglobulin_e.n.01'), Synset('bigram.n.01'), Synset('prolixity.n.01'), Synset('jasper.n.01'), Synset('helios.n.01'), Synset('cybercrime.n.01'), Synset('multicollinearity.n.01'), Synset('agnomen.n.01'), Synset('cyberart.n.01'), Synset('presupposition.n.01'), Synset('misconception.n.01'), Synset('vituperation.n.01'), Synset('substring.n.01'), Synset('yottabyte.n.02'), Synset('decrease.v.02'), Synset('zettabyte.n.02'), Synset('petabit.n.01'), Synset('snit.n.01'), Synset('emboss.v.01'), Synset('horizon.n.02'), Synset('alopex.n.01'), Synset('last_in_first_out.n.01'), Synset('linux.n.01'), Synset('cognitive_semantics.n.01'), Synset('lanseh.n.01'), Synset('plotter.n.02'), Synset('dominance.n.02'), Synset('harem.n.01'), Synset('lolita.n.01'), Synset('ozone.n.01'), Synset('pythia.n.01'), Synset('mime.n.01'), Synset('instantiation.n.01'), Synset('differentiator.n.01'), Synset('swat.v.01'), Synset('fisheye.a.01'), Synset('metadata.n.01'), Synset('dinner_jacket.n.01'), Synset('slob.n.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFrfKX_ydrpJ"
      },
      "source": [
        "n = len(words)\n",
        "wordnetSimilarities = []\n",
        "for i in range(n-1):\n",
        "  print(words[i])\n",
        "  input()\n",
        "  for j in range(i+1,n):\n",
        "    print(words[i].wup_similarity(words[j]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QgHLfanYkTH"
      },
      "source": [
        "Information about three-stage algorithm tree:\n",
        "\n",
        "Depth: 5\n",
        "\n",
        "Average branching factor: 1.84\n"
      ]
    }
  ]
}