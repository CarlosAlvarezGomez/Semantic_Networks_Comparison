{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbYV-x6IvNLO"
      },
      "source": [
        "Imports necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiM3IFMTuxab"
      },
      "source": [
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "import csv\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJnhQkpdgdHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0b39d1-68c6-49dd-ee50-e79a29ccaeb6"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBnW_enIvR6C"
      },
      "source": [
        "Mounts Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayX53S5cvEH9",
        "outputId": "a798839d-e2c4-4cf0-c314-c1094edc58f1"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkKpBZ2yxfGb"
      },
      "source": [
        "Creates threeStageAlgorithm function and getDeltaQ helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLCj8n7NxeYr"
      },
      "source": [
        "def getDeltaQ(key1, key2, communities, node2degrees, graph, m):\n",
        "  deltaQ = 0\n",
        "  for vi in communities[key1]:\n",
        "    neighbors = set(graph.adj[vi])\n",
        "    for vj in communities[key2]:\n",
        "      aij = 1 if vj in neighbors else 0\n",
        "      deltaQ += (aij - node2degrees[vi]*node2degrees[vj]/(2*m))\n",
        "  return deltaQ / (2*m)\n",
        "\n",
        "# Takes in an undirected graph, and returns a dictionary where each key is a\n",
        "# central node of a community and each value is a list of nodes in that\n",
        "# community\n",
        "def threeStageAlgorithm(graph, saveDistances=False, pathToDistances=None, saveSimilarities=False, pathToSimilarities=None):\n",
        "  # Gets the number of nodes along with the degree of each node\n",
        "  minNode = min(graph.nodes)\n",
        "  maxNode = max(graph.nodes)\n",
        "  degrees = [(k,v) for k,v in graph.degree]\n",
        "  degrees.sort(key=lambda x : x[1],reverse=True)\n",
        "  node2degrees = {k:v for k,v in degrees}\n",
        "  n = len(degrees)\n",
        "\n",
        "  # Creates an array of all distances between pairs of nodes if necessary\n",
        "  if pathToDistances == None:\n",
        "    distances = [[None for _ in range(n+1)] for _ in range(n+1)]\n",
        "    for u in tqdm(range(minNode, maxNode), leave=False, desc='Creating distances array'):\n",
        "      for v in range(u+1,maxNode+1):\n",
        "        try:\n",
        "          pathLength = len(nx.shortest_path(graph, u, v))-1\n",
        "          distances[u][v] = pathLength\n",
        "          distances[v][u] = pathLength\n",
        "        except:\n",
        "          continue\n",
        "    \n",
        "    if saveDistances:\n",
        "      with open(pathToDistances, 'wb') as f:\n",
        "        pickle.dump(distances, f)\n",
        "  \n",
        "  # Read in an array of the distances between each pair of nodes\n",
        "  else:\n",
        "    with open(pathToDistances, 'rb') as f:\n",
        "      distances = pickle.load(f)\n",
        "\n",
        "  # Finds the number of pairs that are not connected\n",
        "  notConnected = 0\n",
        "  for u in range(minNode,minNode):\n",
        "    for v in range(u+1,maxNode+1):\n",
        "      if distances[u][v] == None:\n",
        "        notConnected += 1\n",
        "  \n",
        "  connected = n*(n-1) - notConnected\n",
        "\n",
        "  # Finds the sum of the distances\n",
        "  sumDists = 0\n",
        "  for u in range(minNode,maxNode):\n",
        "    for v in range(u+1,maxNode+1):\n",
        "      if distances[u][v] != None:\n",
        "        sumDists += distances[u][v]\n",
        "\n",
        "  # Finds the average distance between every pair of connected nodes\n",
        "  D = 2*sumDists/connected\n",
        "\n",
        "  # Selected all the central nodes and places them in set C0\n",
        "  C0 = set([degrees[0][0]])\n",
        "  for vj, _ in tqdm(degrees[1:], leave=False, desc='Finding central nodes'):\n",
        "    for v in C0:\n",
        "      if distances[v][vj] == None or distances[v][vj] >= D:\n",
        "        C0.add(vj)\n",
        "        break\n",
        "  \n",
        "  # Creates the dictionary of communities\n",
        "  communities = {n:[n] for n in C0}\n",
        "\n",
        "  # Creates an array of similarities between every pair of nodes in a graph if\n",
        "  # necessary\n",
        "  if pathToSimilarities == None:\n",
        "    similarities = [[None for _ in range(n+1)] for _ in range(n+1)]\n",
        "    for u in tqdm(range(minNode,maxNode), leave=False, desc='Creating similarities array'):\n",
        "      uNeighbors = graph.adj[u]\n",
        "      uDegree = node2degrees[u]\n",
        "      for v in range(u+1,maxNode+1):\n",
        "        vNeighbors = graph.adj[v]\n",
        "        vDegree = node2degrees[v]\n",
        "        if (uDegree + vDegree) > 0:\n",
        "          similarities[u][v] = 2*len(list(set(uNeighbors).intersection(set(vNeighbors))))/(uDegree + vDegree)\n",
        "          similarities[v][u] = similarities[u][v]\n",
        "    if saveSimilarities:\n",
        "      with open(pathToSimilarities, 'wb') as f:\n",
        "        pickle.dump(similarities, f)\n",
        "\n",
        "  # Reads in an array of similarities between every pair of nodes in a graph\n",
        "  else:\n",
        "    with open(pathToSimilarities,'rb') as f:\n",
        "      similarities = pickle.load(f)\n",
        "\n",
        "  # Adds each node that has not been labeled yet to a pre-community\n",
        "  notLabeled = set(list(range(minNode,maxNode+1))) - C0\n",
        "  for _ in tqdm(range(len(list(notLabeled))), leave=False, desc='Adding nodes to communities'):\n",
        "    maxSim = 0\n",
        "    for v in C0:\n",
        "      for vj in notLabeled:\n",
        "        if similarities[v][vj] != None and similarities[v][vj] > maxSim:\n",
        "          bestCentralNode = v\n",
        "          bestNewNode = vj\n",
        "          maxSim = similarities[v][vj]\n",
        "    communities[bestCentralNode].append(vj)\n",
        "    notLabeled.remove(vj)\n",
        "\n",
        "  # Deletes distances and similarities arrays because they will no longer be\n",
        "  # used (and they're usually very large)\n",
        "  del distances\n",
        "  del similarities\n",
        "\n",
        "  # Calculates the average degree of every node divided by 2\n",
        "  m = 0\n",
        "  for v in range(minNode,maxNode+1):\n",
        "    m += node2degrees[v]\n",
        "  m /= 2\n",
        "\n",
        "  # Calculates the change in modularity we would get if we merged each pair of\n",
        "  # communities\n",
        "  deltaQs = []\n",
        "  keys = list(communities.keys())\n",
        "  merged = {k:False for k in keys}\n",
        "  for i in tqdm(range(len(keys)-1),leave=False, desc='Calculating changes in modularities'):\n",
        "    key1 = keys[i]\n",
        "    for j in range(i+1, len(keys)):\n",
        "      key2 = keys[j]\n",
        "      deltaQ = getDeltaQ(key1, key2, communities, node2degrees, graph, m)\n",
        "      if deltaQ >= 0:\n",
        "        deltaQs.append([deltaQ, key1, key2])\n",
        "\n",
        "  # Merges communities to maximize modularity\n",
        "  deltaQs.sort(key=lambda x : x[0], reverse=True)\n",
        "  length = len(deltaQs)\n",
        "  idx = 0\n",
        "  for idx in tqdm(range(length), leave=False, desc='Merging communities'):\n",
        "    key1 = deltaQs[idx][1]\n",
        "    key2 = deltaQs[idx][2]\n",
        "    if not(merged[key1]) and not(merged[key2]):\n",
        "      merged[key1] = True\n",
        "      merged[key2] = True\n",
        "      communities[key1] = communities[key1] + communities[key2]\n",
        "      communities.pop(key2)\n",
        "  \n",
        "  new_communities = {}\n",
        "  for idx, item in enumerate(communities.items()):\n",
        "    new_communities[idx] = item[1]\n",
        "\n",
        "  return new_communities"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgylHqEmvUS6"
      },
      "source": [
        "Reads docmap file and generates a list of all documents. Each document is represented as a list with 3 elements: the node index, the document id, and the document name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaBDrXPcvG6W"
      },
      "source": [
        "docMapPath = 'drive/My Drive/CS 6850 Final Project/Data/docmap'\n",
        "file=open(docMapPath,\"r\")\n",
        "docMapStrings = file.read().split('\\n')\n",
        "docMapStrings = list(map(lambda x : x.split('\\t'), docMapStrings))\n",
        "docMap = []\n",
        "for index, line in enumerate(docMapStrings):\n",
        "  docMap.append([index+1, int(line[0]), line[1]])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf6NlhiwvhEH"
      },
      "source": [
        "Reads pair_doc file and generate a list of all the edges. Each edge is represented as a list of 2 elements: the source node and the destination node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqPscqX7vIiQ"
      },
      "source": [
        "edgesPath = 'drive/My Drive/CS 6850 Final Project/Data/pair_doc'\n",
        "file=open(edgesPath,\"r\")\n",
        "edgesStrings = file.read().split('\\n')\n",
        "edgesStrings = list(map(lambda x : x.split(' '), edgesStrings))\n",
        "edgesStrings=  list(filter(lambda x : x != [''], edgesStrings))\n",
        "edges = []\n",
        "for line in edgesStrings:\n",
        "  edges.append([int(line[0]), int(line[1])])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdSiBTxevk6f"
      },
      "source": [
        "Checks if the node indices are 0-indexed or 1-indexed by seeing if 0 is used in the edges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAQtEYxxvJM9"
      },
      "source": [
        "for edge in edges:\n",
        "  if 0 in edge:\n",
        "    print(edge)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF1uVp2dvnxA"
      },
      "source": [
        "Creates a directed graph, directedG, and an undirected graph, undirectedG1. directedG will contains all edges in the original dataset. undirectedG1 will only contain edges between nodes u and v if there is one edge going from u to v and one edge going from v to u in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C_1QFUPvoma"
      },
      "source": [
        "directedG = nx.DiGraph()\n",
        "undirectedG1 = nx.Graph()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK-Vn6eIvqc5"
      },
      "source": [
        "Adds all nodes to both graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPfVJwv3vsts"
      },
      "source": [
        "for doc in docMap:\n",
        "  directedG.add_node(doc[0], id=doc[1], name=doc[2])\n",
        "  undirectedG1.add_node(doc[0], id=doc[1], name=doc[2])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiAmXwMZvvE_"
      },
      "source": [
        "Adds all edges to directedG, and adds necessary edges to undirectedG1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3A9S6-UvxL5"
      },
      "source": [
        "edgesSeen = set()\n",
        "for edge in edges:\n",
        "  u = edge[0]\n",
        "  v = edge[1]\n",
        "  directedG.add_edge(u, v)\n",
        "  if str([v, u]) in edgesSeen:\n",
        "    undirectedG1.add_edge(u, v)\n",
        "  else:\n",
        "    edgesSeen.add(str(edge))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJAgsgyzv1ML"
      },
      "source": [
        "Creates another undirected graph, undirectedG2. This graph contains all the edges from directedG, except that they have been converted to undirected edges, and duplicate undirected edges between nodes were removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCT22hCLv13q"
      },
      "source": [
        "undirectedG2 = directedG.to_undirected()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks that the number of edges in all graphs ia correct"
      ],
      "metadata": {
        "id": "X5mrm8giW4GQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCIoaXy5v3tY"
      },
      "source": [
        "assert(directedG.number_of_edges() == undirectedG1.number_of_edges() + undirectedG2.number_of_edges())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDZmXcOkv6c4"
      },
      "source": [
        "Makes sure each graph has the correct number of edges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdHuxscrv81Q"
      },
      "source": [
        "edgesSeen = set()\n",
        "undirectedG1Edges = undirectedG1.edges\n",
        "undirectedG2Edges = undirectedG2.edges\n",
        "for edge in directedG.edges:\n",
        "  u = edge[0]\n",
        "  v = edge[1]\n",
        "  assert (edge in undirectedG2Edges)\n",
        "  if str((v,u)) in edgesSeen:\n",
        "    assert (edge in undirectedG1Edges)\n",
        "  else:\n",
        "    edgesSeen.add(str(edge))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3jP6X8812Yx"
      },
      "source": [
        "def generateLayerGraph(communities, childGraph):\n",
        "  graph = nx.Graph()\n",
        "  keys = list(communities.keys())\n",
        "  for k in keys:\n",
        "    graph.add_node(k)\n",
        "  \n",
        "  length = len(keys)\n",
        "  for u in range(length-1):\n",
        "    key1 = keys[u]\n",
        "    uNodes = set(communities[key1])\n",
        "    for v in range(u+1,length):\n",
        "      key2 = keys[v]\n",
        "      for vNode in communities[key2]:\n",
        "        vNeighbors = set(childGraph.adj[vNode])\n",
        "        if uNodes.intersection(vNeighbors) != set():\n",
        "          graph.add_edge(u,v)\n",
        "          break\n",
        "  \n",
        "  return graph"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QDo3qsvLTEq"
      },
      "source": [
        "def combineLayers(layers, communities):\n",
        "  graph = nx.DiGraph()\n",
        "  n = len(layers)\n",
        "  root = str((n, 0))\n",
        "  graph.add_node(root)\n",
        "  layers.reverse()\n",
        "  communities.reverse()\n",
        "  treeLayer = n\n",
        "  for idx, curGraph in enumerate(layers[1:-1]):\n",
        "    treeLayer = treeLayer - 1\n",
        "    for node in curGraph.nodes:\n",
        "      graph.add_node(str((treeLayer, node)))\n",
        "\n",
        "    curCommunities= communities[idx]\n",
        "    for k, v in curCommunities.items():\n",
        "      for node in v:\n",
        "        graph.add_edge(str((treeLayer+1, k)), str((treeLayer, node)))\n",
        "    \n",
        "  for node in layers[-1].nodes:\n",
        "    graph.add_node(node)\n",
        "  \n",
        "  curCommunities = communities[-1]\n",
        "  for k, v in curCommunities.items():\n",
        "    for node in v:\n",
        "      graph.add_edge(str((2, k)), node)\n",
        "\n",
        "  return graph, root"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BewZgDcRLTy0"
      },
      "source": [
        "Creates a function that takes in a list of layers and communitites and creates a tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwd9La_xwtOb"
      },
      "source": [
        "def generateTree(leafGraph, algorithm):\n",
        "  layers = [leafGraph]\n",
        "  curCommunities = algorithm(leafGraph)\n",
        "  communities = [curCommunities]\n",
        "  layers.append(generateLayerGraph(communities[0], leafGraph))\n",
        "  prevLayerLength = len(list(layers[0].nodes))\n",
        "  iteration = 0\n",
        "  while len(communities[-1].items()) > 1 and len(communities[-1].items()) < prevLayerLength:\n",
        "    iteration += 1\n",
        "    print(iteration)\n",
        "    prevLayerLength = len(list(layers[-1].nodes))\n",
        "    communities.append(algorithm(layers[-1]))\n",
        "    layers.append(generateLayerGraph(communities[-1], layers[-1]))\n",
        "\n",
        "  tree, root = combineLayers(layers, communities)\n",
        "\n",
        "  return tree, root, layers, communities"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = list(TSAGraph.nodes)[0]\n",
        "while set(TSAGraph.predecessors(root)) != set():\n",
        "  root = set(TSAGraph.predecessors(root))[0]"
      ],
      "metadata": {
        "id": "qEuYCuEHBHCf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(root)\n",
        "print(TSAGraph.degree[root])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqU251BNBhOK",
        "outputId": "17e8e93e-9096-4f6b-a9b2-c5737694237f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 0)\n",
            "998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gets TSA graph"
      ],
      "metadata": {
        "id": "jBxMyUM5TZYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  with open('drive/My Drive/CS 6850 Final Project/TSA/layers.pkl', 'rb') as f:\n",
        "    layers = pickle.load(f)\n",
        "  with open('drive/My Drive/CS 6850 Final Project/TSA/communities.pkl', 'rb') as f:\n",
        "    communities = pickle.load(f)\n",
        "  with open('drive/My Drive/CS 6850 Final Project/TSA/tree.pkl', 'rb') as f:\n",
        "    TSAGraph = pickle.load(f)\n",
        "  # with open('drive/My Drive/CS 6850 Final Project/TSA/root.pkl', 'rb') as f:\n",
        "  #   root = pickle.load(f)\n",
        "except:\n",
        "  stop('error')"
      ],
      "metadata": {
        "id": "mRbt0BB7RC4Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gets Wikipedia category graph"
      ],
      "metadata": {
        "id": "hf2mkrbp4lEQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsZbxke-OZTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb118f1-b2d4-471f-c251-525b8ee60944"
      },
      "source": [
        "with open('drive/My Drive/CS 6850 Final Project/wiki_category_trimmed_bsf_tree.json') as f:\n",
        "    trimmed_edges_list = json.load(f)\n",
        "\n",
        "WikiGraph = nx.DiGraph()\n",
        "for n1, n2 in tqdm(trimmed_edges_list):\n",
        "    WikiGraph.add_edge(n1, n2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14868/14868 [00:00<00:00, 67665.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following part computes the semantic relatedness scores using the trees we have"
      ],
      "metadata": {
        "id": "scMbf1Bh4XlA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH0wbJwxRjXK"
      },
      "source": [
        "# Loads a dictionary from names of articles to their ID in the Wikipedia graph\n",
        "with open('drive/My Drive/CS 6850 Final Project/updates_page_names.json') as f:\n",
        "    names2Wiki_id = json.load(f)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvYahCtdSEaP"
      },
      "source": [
        "# Get a list of all article names\n",
        "all_names = list(map(lambda x : x.replace('_', ' '), names2Wiki_id.keys()))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MmyDwIWY9Fz"
      },
      "source": [
        "# Creates a dictionary from names of articles to their ID in the TSA graph\n",
        "names = list(map(lambda x : x[2].replace('_', ' '), docMap))\n",
        "names2TSA_id = {name: names.index(name)+1 for name in all_names}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGQ_XS-8bjPk"
      },
      "source": [
        "words = list(map(lambda x : wn.synsets(x)[0], all_names))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqRxcSoMtAvy"
      },
      "source": [
        "word2Name = {word:name for name, word in zip(all_names, words)}"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zODrdRLqtgAP"
      },
      "source": [
        "def wup_similarity(graph, node_id1, node_id2):\n",
        "  root = node_id1\n",
        "  while list(graph.predecessors(root)) != []:\n",
        "    root = list(graph.predecessors(root))[0]\n",
        "  node1_paths = list(nx.algorithms.all_simple_paths(graph, source=root, target=node_id1))\n",
        "  node2_paths = list(nx.algorithms.all_simple_paths(graph, source=root, target=node_id2))\n",
        "  lcsDepth = 0\n",
        "  for node1_path in node1_paths:\n",
        "    for node2_path in node2_paths:\n",
        "      idx = 0\n",
        "      n = min(len(node1_path), len(node2_path))\n",
        "      while idx < n and node1_path[idx] == node2_path[idx]:\n",
        "        idx += 1\n",
        "      commonParent = node1_path[idx-1]\n",
        "      commonParentDepth = nx.algorithms.shortest_path_length(graph, source=root, target=commonParent)\n",
        "      if commonParentDepth > lcsDepth:\n",
        "        lcsDepth = commonParentDepth\n",
        "  \n",
        "  node1Depth = nx.algorithms.shortest_path_length(graph, source=root, target=node_id1)\n",
        "  node2Depth = nx.algorithms.shortest_path_length(graph, source=root, target=node_id2)\n",
        "  return 2*lcsDepth/(node1Depth + node2Depth)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getModularity(graph, communities):\n",
        "  modularity = 0\n",
        "  m = len(list(graph.edges))\n",
        "  for _, nodes in communities.items():\n",
        "    n_nodes = len(nodes)\n",
        "    if n_nodes > 1:\n",
        "      for i in range(n_nodes-1):\n",
        "        neighbors = set(graph.adj[nodes[i]])\n",
        "\n",
        "        for j in range(i+1, n_nodes):\n",
        "          a = 1 if nodes[j] in neighbors else 0\n",
        "          modularity += a - graph.degree[nodes[i]]*graph.degree[nodes[j]]/(2*m)\n",
        "  modularity /= 2*m\n",
        "  return modularity"
      ],
      "metadata": {
        "id": "LDh7WlTjxX_X"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for graph, comm in zip(layers, communities):\n",
        "  print(getModularity(graph, comm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugKNgWlLzkbx",
        "outputId": "639876c0-003a-4348-ca50-06698ae84109"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07388390396836707\n",
            "0.05413942167197443\n",
            "0.028030101426515498\n",
            "0.030370931684179387\n",
            "0.0327618816000214\n",
            "0.06311386516671168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comm =  communities[-2]\n",
        "sizes = {}\n",
        "for v in comm.values():\n",
        "  size = len(v)\n",
        "  if size in sizes:\n",
        "    sizes[size] += 1\n",
        "  else:\n",
        "    sizes[size] = 1\n",
        "# new_sizes = {v:k for k, v in sizes.items()}\n",
        "# x_min = min(new_sizes.values())\n",
        "# x_max = max(new_sizes.values())\n",
        "# counts = []\n",
        "# for i in range(x_min, x_max+1):\n",
        "#   if i in new_sizes:\n",
        "#     counts.append(new_sizes[i])\n",
        "#   else:\n",
        "#     counts.append(0)\n",
        "all_items = list(sizes.items())\n",
        "k = list(map(lambda x : x[0], all_items))\n",
        "v = list(map(lambda x : x[1], all_items))\n",
        "plt.bar(v, k)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "noCp7p3P3GQ3",
        "outputId": "65572373-f471-4e99-d631-e30fb6363aeb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 4 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFrfKX_ydrpJ"
      },
      "source": [
        "try:\n",
        "  with open('drive/My Drive/CS 6850 Final Project/wikiSimilarities.pkl', 'rb') as f:\n",
        "    WikiSimilarities = pickle.load(f)\n",
        "  with open('drive/My Drive/CS 6850 Final Project/wordnetSimilarities.pkl', 'rb') as f:\n",
        "    wordnetSimilarities = pickle.load(f)\n",
        "  with open('drive/My Drive/CS 6850 Final Project/TSASimilarities.pkl', 'rb') as f:\n",
        "    TSASimilarities = pickle.load(f)\n",
        "except:\n",
        "  print('error')\n",
        "  n = len(words)\n",
        "  wordnetSimilarities = []\n",
        "  TSASimilarities = []\n",
        "  WikiSimilarities = []\n",
        "  for i in tqdm(range(n-1), leave=False):\n",
        "    name1 = word2Name[words[i]]\n",
        "    TSA1 = names2TSA_id[name1]\n",
        "    Wiki1 = names2Wiki_id[name1.replace(' ', '_')]\n",
        "    for j in range(i+1,n):\n",
        "      wordnetSim = words[i].wup_similarity(words[j])\n",
        "      if wordnetSim != None:\n",
        "        wordnetSimilarities.append(wordnetSim)\n",
        "        name2 = word2Name[words[j]]\n",
        "        TSA2 = names2TSA_id[name2]\n",
        "        Wiki2 = names2Wiki_id[name2.replace(' ', '_')]\n",
        "        TSASimilarities.append(wup_similarity(TSAGraph, TSA1, TSA2))\n",
        "        WikiSimilarities.append(wup_similarity(WikiGraph, Wiki1, Wiki2))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docMap[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRddrb8m_G44",
        "outputId": "939a5000-1393-4dfd-c186-2fe948715672"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 5323, 'Computer_science'],\n",
              " [2, 169633, 'Topic_outline_of_computer_science'],\n",
              " [3, 328784, 'Computer_scientist'],\n",
              " [4, 900765, 'Program_(mathematical_object)'],\n",
              " [5, 1180800, 'Overlapping_subproblem']]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/My Drive/CS 6850 Final Project/CIDLPA_tree.pkl', 'rb') as f:\n",
        "    CIDLPA_tree = pickle.load(f)"
      ],
      "metadata": {
        "id": "FYK07ore96g3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(words)\n",
        "CIDLPASimilarities = []\n",
        "for i in tqdm(range(n-1), leave=False):\n",
        "  name1 = word2Name[words[i]]\n",
        "  TSA_id1 = names2TSA_id[name1.replace(' ', '_')]\n",
        "  print(TSA_id1)\n",
        "  for j in range(i+1,n):\n",
        "    wordnetSim = words[i].wup_similarity(words[j])\n",
        "    if wordnetSim != None:\n",
        "      name2 = word2Name[words[j]]\n",
        "      TSA_id2 = names2TSA_id[name2.replace(' ', '_')]\n",
        "      print(TSA_id2)\n",
        "      example = wup_similarity(TSAGraph, TSA_id1, TSA_id2)\n",
        "      CIDLPASimilarities.append(wup_similarity(CIDLPA_tree, Wiki1, Wiki2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "8EIhJ5Pm-MB7",
        "outputId": "996be7c0-e1a9-4482-b299-ff45d4b3b5b3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/158 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5942\n",
            "12914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "error",
          "ename": "NetworkXNoPath",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNetworkXNoPath\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-dce5815351e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTSA_id2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwup_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTSAGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTSA_id1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTSA_id2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mCIDLPASimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCIDLPA_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWiki1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWiki2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-3f862c42d02f>\u001b[0m in \u001b[0;36mwup_similarity\u001b[0;34m(graph, node_id1, node_id2)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mnode1Depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortest_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_id1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mnode2Depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortest_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_id2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlcsDepth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode1Depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnode2Depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/networkx/algorithms/shortest_paths/generic.py\u001b[0m in \u001b[0;36mshortest_path_length\u001b[0;34m(G, source, target, weight, method)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;31m# Find shortest source-target path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"unweighted\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional_shortest_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"dijkstra\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36mbidirectional_shortest_path\u001b[0;34m(G, source, target)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# call helper to do the real work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bidirectional_pred_succ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/networkx/algorithms/shortest_paths/unweighted.py\u001b[0m in \u001b[0;36m_bidirectional_pred_succ\u001b[0;34m(G, source, target)\u001b[0m\n\u001b[1;32m    290\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetworkXNoPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No path between {source} and {target}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNetworkXNoPath\u001b[0m: No path between (6, 10832) and 12914."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb9GpT8hzQcz"
      },
      "source": [
        "allSimilarities = np.array([wordnetSimilarities, TSASimilarities, WikiSimilarities])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsFRQXJ20KYF"
      },
      "source": [
        "np.corrcoef(allSimilarities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(wordnetSimilarities, TSASimilarities)"
      ],
      "metadata": {
        "id": "HM-SqHQdmXPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(wordnetSimilarities, WikiSimilarities)"
      ],
      "metadata": {
        "id": "BhuFfiC17Z4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(WikiSimilarities, TSASimilarities)"
      ],
      "metadata": {
        "id": "FFrJ3Tdv8wCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/My Drive/CS 6850 Final Project/wikiSimilarities.pkl', 'wb') as f:\n",
        "  pickle.dump(WikiSimilarities, f)\n",
        "with open('drive/My Drive/CS 6850 Final Project/wordnetSimilarities.pkl', 'wb') as f:\n",
        "  pickle.dump(wordnetSimilarities, f)\n",
        "with open('drive/My Drive/CS 6850 Final Project/TSASimilarities.pkl', 'wb') as f:\n",
        "  pickle.dump(TSASimilarities, f)"
      ],
      "metadata": {
        "id": "B87uscOSm699"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QgHLfanYkTH"
      },
      "source": [
        "Information about three-stage algorithm tree:\n",
        "\n",
        "Depth: 5\n",
        "\n",
        "Average branching factor: 1.84\n"
      ]
    }
  ]
}